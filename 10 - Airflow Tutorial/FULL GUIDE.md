# Source

- (video kinda deprecated) https://youtu.be/K9AnJ9_ZAXE
- https://globant.udemy.com/course/the-complete-hands-on-course-to-master-apache-airflow
- https://marclamberti.com/

# Install (linux only) (it didn‚Äôt work neither)

## Instalar Python 3.12 en ubuntu

```bash
sudo apt update
sudo apt upgrade
sudo apt install -y software-properties-common
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install -y python3.12
python3.12 --version  # Verificar instalaci√≥n
sudo apt install python3.12 python3.12-venv
sudo apt install python3-pip
# (Opcional) Crear un enlace simb√≥lico
sudo ln -s /usr/bin/python3.12 /usr/bin/python
```

## Instalar Airflow

https://github.com/apache/airflow?tab=readme-ov-file#installing-from-pypi

Tienes que copiar el comando de pip y pues ejecutarlo adentro de un entorno virtual. Ubuntu solo te deja usar entornos virtuales.

```bash
# Crear un entorno virtual
python3.12 -m venv venv

# Activar el entorno virtual
source venv/bin/activate

# Instalar Apache Airflow
pip install 'apache-airflow==2.10.5' --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.10.5/constraints-3.12.txt"

# Verificar la instalaci√≥n
airflow version

# Desactivar el entorno virtual cuando termines
deactivate
```

- Windows again, deprecated
    
    Al final del comando, puedes especificar tu versi√≥n de python.
    
    ```bash
    py -3.12 -m venv .venv
    .\.venv\Scripts\activate
    ```
    
    El pc de la empresa no deja crear venvs as√≠ que vale verga
    
    Dig√°mosle a pip que instale la wea en espec√≠ficamente mi python 3.12 (el m√°ximo que se pudo cuando hice esto)
    
    ```bash
    py -3.12 -m pip install 'apache-airflow==2.10.5' --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.10.5/constraints-3.12.txt"
    ```
    
    El equivalente de `py -3.12` que es de windows, en linux, ser√≠a `python3.12`
    
    Recuerda poner el directorio de los scripts de PIP de tu python 3.12 en el PATH. `C:\Users\esteban.vergara\AppData\Local\Programs\Python\Python312\Scripts`
    

## AIRFLOW_HOME (Deprecated, windows)

En Windows, la forma de establecer variables de entorno es diferente a la de Unix/Linux. Si deseas establecer la variable de entorno `AIRFLOW_HOME` en Windows, puedes hacerlo de varias maneras. Aqu√≠ te muestro dos m√©todos: usando la l√≠nea de comandos (CMD) y PowerShell.

### Opci√≥n 1: Usando la l√≠nea de comandos (CMD)

1. Abre el s√≠mbolo del sistema (CMD).
2. Ejecuta el siguiente comando para establecer la variable de entorno:
    
    ```bash
    set AIRFLOW_HOME=.
    
    ```
    
    Este comando establece `AIRFLOW_HOME` en el directorio actual. Sin embargo, esta variable solo estar√° activa en la ventana de CMD actual.
    
3. Si deseas que esta variable persista incluso despu√©s de cerrar la ventana, puedes establecerla de forma permanente con:
    
    ```bash
    setx AIRFLOW_HOME "."
    
    ```
    

### Opci√≥n 2: Usando PowerShell

1. Abre PowerShell.
2. Ejecuta el siguiente comando para establecer la variable de entorno:
    
    ```powershell
    $env:AIRFLOW_HOME="."
    
    ```
    
    Al igual que en CMD, esta variable solo estar√° activa en la sesi√≥n actual de PowerShell.
    
3. Para establecerla de forma permanente en PowerShell, utiliza el siguiente comando:
    
    ```powershell
    [System.Environment]::SetEnvironmentVariable("AIRFLOW_HOME", ".", "User")
    
    ```
    

### Verificar la Variable de Entorno

Para asegurarte de que la variable se ha establecido correctamente, puedes usar el siguiente comando en CMD:

```bash
echo %AIRFLOW_HOME%

```

O en PowerShell:

```powershell
echo $env:AIRFLOW_HOME

```

### Resumen

- **CMD**: Usa `set` para variables temporales y `setx` para variables permanentes.
- **PowerShell**: Usa `$env:` para variables temporales y `[System.Environment]::SetEnvironmentVariable` para variables permanentes.

Siguiendo estos pasos, podr√°s establecer la variable `AIRFLOW_HOME` en tu entorno de Windows.

## AIRFLOW_HOME

La idea era hacer `export AIRFLOW_HOME=.` y luego correr `airflow db init` , el problema es que todo eso parece viejo y deprecado, y sqlite tira problema por usar rutas relativas, y b√°sicamente no te deja usarlo descargando todo en local, as√≠ que yo prefiero tirar todo esto del video a la basura y mejor hacerlo con Docker desde el inicio.

# Install using Docker, no matter where

https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html

https://airflow.apache.org/docs/apache-airflow/2.5.1/docker-compose.yaml

https://airflow.apache.org/docs/apache-airflow/2.10.5/docker-compose.yaml

Hi there üëã

It's time to install Apache Airflow. As you are going to see, it's very simple.

**Prerequisites**

First, make sure you have installed Docker Desktop and Visual Studio. If not, take a look at these links:

- [Get Docker](https://docs.docker.com/get-docker/)
- [Get Visual Studio Code](https://code.visualstudio.com/download)

Docker needs privilege rights to work, make sure you have them.

**Follow the documentation first.**

If you have trouble to install these tools, here are some videos to help you

- [Install Docker on Windows 10](https://www.youtube.com/watch?v=lIkxbE_We1I&ab_channel=JamesStormes)
- [Install Docker on Windows 10 with WSL 2](https://www.youtube.com/watch?v=h0Lwtcje-Jo&ab_channel=BeachcastsProgrammingVideos)
- [Install Docker on Windows 11](https://youtu.be/6k1CyA5zYgg?t=249)

**Install Apache Airflow with Docker**

1. Create a folder¬†**materials**¬†in your¬†**Documents**
2. In this folder, download the following file:¬†[docker compose file](https://airflow.apache.org/docs/apache-airflow/2.5.1/docker-compose.yaml)
3. If you right-click on the file and save it, you will end up with docker-compose.yaml.txt. Remove the .txt and keep docker-compose.yaml
4. Open your terminal or CMD and go into¬†**Documents/materials**
5. Open Visual Studio Code by typing the command:¬†`code .`
6. You should have something like this
    
    ![](https://img-c.udemycdn.com/redactor/raw/article_lecture/2022-05-19_08-58-14-75ad4de4f9c3fa386546708ce86db04a.png)
    
7. Right click below docker-compose.yml and create a new file¬†`.env`¬†(don't forget the dot before env)
8. In this file add the following lines
    
    
        `1. AIRFLOW_IMAGE_NAME=apache/airflow:2.4.2
        2. AIRFLOW_UID=50000`
    
    1. 1. AIRFLOW_IMAGE_NAME=apache/airflow:2.4.2
    2. 2. AIRFLOW_UID=50000
    
    and save the file
    
    ![](https://img-c.udemycdn.com/redactor/raw/article_lecture/2022-05-19_08-58-15-94fb1fff7e92c0ca1378ec7421e0f650.png)
    
9. Go at the top bar of Visual Studio Code -> Terminal -> New Terminal
    
    ![](https://img-c.udemycdn.com/redactor/raw/article_lecture/2022-05-19_08-58-15-c82d370ca2d8cf4b8eecc5337650dca6.png)
    
10. In your new terminal at the bottom of Visual Studio Code, type the command¬†`docker-compose up -d`¬†and hit¬†`ENTER`
    
    ![](https://img-c.udemycdn.com/redactor/raw/article_lecture/2022-05-19_08-58-15-8a8428697d23ce89a6cd8af85881523c.png)
    
11. You will see many lines scrolled, wait until it's done. Docker is downloading Airflow to run it. It can take up to 5 mins depending on your connection. If Docker raises an error saying it can't download the docker image, ensure you are not behind a proxy/vpn or corporate network. You may need to use your personal connection to make it work. At the end, you should end up with something like this:
    
    ![](https://img-c.udemycdn.com/redactor/raw/article_lecture/2022-05-19_08-58-15-1d0b2c401e8b14e1d23ec2024eaf0eef.png)
    

Well done, you've just installed Apache Airflow with Docker! üéâ

Open your web browser and go to¬†`localhost:8080`

![](https://img-c.udemycdn.com/redactor/raw/article_lecture/2022-05-19_08-58-15-8e7bab039ef558f46a3263d851649e95.png)

### Troubleshoots

- > If you don't see this page, make sure you have nothing already running on the port 8080

Also, go back to your terminal on Visual Studio Code and check your application with¬†`docker-compose ps`

All of your "containers" should be healthy as follow:

![](https://img-c.udemycdn.com/redactor/raw/article_lecture/2022-05-19_08-58-15-414bbbd37e901753ea738f68380ef573.png)

If a container is not healthy. You can check the logs with¬†`docker logs materials_name_of_the_container`

Try to spot the error; once you fix it, restart Airflow with¬†`docker-compose down`¬†then¬†`docker-compose up -d`

and wait until your container states move from starting to healthy.

- > If you see this error

![](https://img-c.udemycdn.com/redactor/raw/article_lecture/2023-03-06_08-28-42-9884e885f01d16f91bd4a0f03ed267db.png)

remove your volumes with¬†`docker volume prune`¬†and run¬†`docker-compose up -d`¬†again

- > If you see that airflow-init docker container has exited, that's normal :)

If you still have trouble, reach me on the Q/A with your error.

# Components

Apache Airflow es una plataforma de orquestaci√≥n de flujos de trabajo que permite programar y monitorizar tareas. A continuaci√≥n, se presenta un resumen de sus componentes principales:

### 1. UI (Interfaz de Usuario)

La UI es la interfaz gr√°fica donde los usuarios pueden visualizar y gestionar los flujos de trabajo (DAGs). Permite a los usuarios iniciar, detener y monitorear la ejecuci√≥n de las tareas, as√≠ como revisar registros y estados de ejecuci√≥n. La UI es fundamental para la interacci√≥n del usuario con Airflow.

### 2. Scheduler

El Scheduler es el componente responsable de programar las tareas en los DAGs. Monitorea los DAGs y determina cu√°ndo deben ejecutarse las tareas bas√°ndose en sus dependencias y cronogramas. El Scheduler es crucial para asegurar que las tareas se ejecuten en el orden correcto y en el momento adecuado.

### 3. Meta Database

La Meta Database es donde Airflow almacena la informaci√≥n sobre los DAGs, las tareas, los estados de ejecuci√≥n y otros metadatos. Esta base de datos es fundamental para el funcionamiento de Airflow, ya que permite el seguimiento del estado de las tareas y su programaci√≥n. Generalmente se utiliza una base de datos relacional como PostgreSQL o MySQL.

### 4. Triggerer

El Triggerer es un componente que permite ejecutar tareas basadas en eventos. Se utiliza para manejar tareas que dependen de condiciones externas o eventos, como esperar a que un archivo llegue a un sistema de archivos o a que se complete una tarea en otro sistema. Esto permite una mayor flexibilidad y control sobre la ejecuci√≥n de flujos de trabajo.

### 5. The Executor

El Executor es responsable de ejecutar las tareas programadas. Airflow ofrece diferentes tipos de ejecutores (como LocalExecutor, CeleryExecutor, entre otros) que gestionan c√≥mo y d√≥nde se ejecutan las tareas. La elecci√≥n del ejecutor puede afectar el rendimiento y la escalabilidad del flujo de trabajo.

### 6. The Queue

La Queue es un sistema de mensajer√≠a que permite a los componentes de Airflow comunicarse entre s√≠. Las tareas se env√≠an a la cola para ser ejecutadas por los workers. Es un elemento clave en arquitecturas distribuidas, ya que permite la separaci√≥n de las tareas y la ejecuci√≥n en paralelo.

### 7. The Worker

Los Workers son los procesos que ejecutan las tareas. Dependiendo del ejecutor utilizado, pueden ser procesos locales o distribuidos en un cl√∫ster. Los workers toman las tareas de la cola y las ejecutan, reportando el estado de la ejecuci√≥n de vuelta al Scheduler y a la Meta Database.

Estos componentes trabajan en conjunto para facilitar la orquestaci√≥n de flujos de trabajo, asegurando que las tareas se ejecuten de manera eficiente y en el orden correcto.

# Concepts

P√°gina para ver los operators disponibles: https://www.astronomer.io/

[Astronomer: The Best Place to Run Apache Airflow¬Æ](https://www.astronomer.io/)

Aqu√≠ tienes un resumen de los conceptos clave en Apache Airflow:

### 1. DAG (Directed Acyclic Graph)

Un DAG es una representaci√≥n gr√°fica de un flujo de trabajo en Airflow. Consiste en un conjunto de tareas (tasks) organizadas en un gr√°fico ac√≠clico dirigido, lo que significa que no hay ciclos y cada tarea puede depender de otras. Los DAGs definen la estructura y el orden en el que se deben ejecutar las tareas, as√≠ como sus dependencias y condiciones de programaci√≥n.

### 2. Operator

Un Operator es un componente que define una tarea espec√≠fica dentro de un DAG. En Airflow, los Operators son clases que encapsulan la l√≥gica de ejecuci√≥n de una tarea, y pueden ser de diferentes tipos seg√∫n la acci√≥n que realizan, como ejecutar un script, realizar una consulta a una base de datos, enviar un correo electr√≥nico, entre otros. Algunos ejemplos de Operators son `BashOperator`, `PythonOperator`, `EmailOperator`, etc.

### 3. Task / Task Instance

- **Task**: Una tarea es una unidad de trabajo en un DAG. Cada tarea se define utilizando un Operator y tiene un nombre √∫nico dentro del DAG. Las tareas son los bloques de construcci√≥n que se ejecutan durante la orquestaci√≥n del flujo de trabajo.
- **Task Instance**: Una instancia de tarea es una representaci√≥n espec√≠fica de una tarea en un momento dado. Cada vez que se ejecuta una tarea en un DAG, se crea una nueva instancia de tarea que tiene un estado asociado (por ejemplo, "en ejecuci√≥n", "completada", "fallida"). Las instancias de tarea permiten a Airflow hacer un seguimiento del estado y la ejecuci√≥n de cada tarea en diferentes ejecuciones del DAG.

### 4. Workflow

Un workflow (flujo de trabajo) en Airflow es el conjunto completo de tareas y sus interacciones definidas en un DAG. Representa el proceso completo que se debe llevar a cabo para lograr un objetivo espec√≠fico. Los workflows pueden incluir una secuencia de tareas dependientes, tareas paralelas, y l√≥gica condicional, lo que permite construir flujos de trabajo complejos y robustos para la orquestaci√≥n de datos y tareas.

Estos conceptos son fundamentales para entender c√≥mo se estructuran y ejecutan los flujos de trabajo en Apache Airflow.

## Res√∫men

*What should you keep in mind after what you've learned?*

- Airflow is an orchestrator, not a processing framework. Process your gigabytes of data outside of Airflow (i.e. You have a Spark cluster, you use an operator to execute a Spark job, and the data is processed in Spark).
- A DAG is a data pipeline, an Operator is a task.
- An Executor defines how your tasks are executed, whereas a worker is a process executing your task
- The Scheduler schedules your tasks, the web server serves the UI, and the database stores the metadata of Airflow.

# Airflow NO

## No es‚Ä¶

Tienes raz√≥n; Apache Airflow no es un sistema de procesamiento de datos, un sistema de streaming ni un sistema de almacenamiento de datos. A continuaci√≥n, explico estas diferencias y el enfoque de Airflow en la orquestaci√≥n de flujos de trabajo por lotes (batch):

### 1. No es un Sistema de Procesamiento de Datos

Apache Airflow es una herramienta de orquestaci√≥n que permite gestionar y programar tareas, pero no realiza el procesamiento de datos en s√≠. En cambio, se utiliza para coordinar tareas que pueden incluir el procesamiento de datos mediante otras herramientas o sistemas. Por ejemplo, puedes usar Airflow para orquestar un flujo de trabajo que incluya tareas de extracci√≥n, transformaci√≥n y carga (ETL) que se ejecutan en sistemas de procesamiento de datos como Apache Spark, Hadoop o bases de datos SQL.

### 2. Enfoque en Batch

Airflow est√° dise√±ado principalmente para flujos de trabajo por lotes (batch processing). Esto significa que se utiliza para programar y ejecutar tareas en intervalos regulares o en funci√≥n de eventos espec√≠ficos, en lugar de procesar datos en tiempo real. Los flujos de trabajo en Airflow se ejecutan en funci√≥n de una programaci√≥n establecida, lo que permite a los usuarios manejar grandes cantidades de datos en lotes, en lugar de procesar datos continuamente como en un sistema de streaming.

### 3. No es un Sistema de Almacenamiento de Datos

Airflow no est√° destinado a almacenar datos. En su lugar, act√∫a como un coordinador que gestiona la ejecuci√≥n de tareas que pueden leer o escribir en sistemas de almacenamiento. Por ejemplo, un flujo de trabajo de Airflow podr√≠a incluir tareas que extraen datos de una base de datos, procesan esos datos y luego los escriben en un sistema de almacenamiento como Amazon S3, Google Cloud Storage o una base de datos. La Meta Database de Airflow solo se utiliza para almacenar metadatos sobre las ejecuciones de los flujos de trabajo y el estado de las tareas, no para almacenar los datos que se procesan en esos flujos de trabajo.

### Conclusi√≥n

En resumen, Apache Airflow es una herramienta de orquestaci√≥n de flujos de trabajo que permite a los usuarios coordinar y programar tareas de procesamiento de datos por lotes, pero no realiza el procesamiento de datos en s√≠, no opera en tiempo real (streaming) y no almacena datos. Su enfoque en la orquestaci√≥n le permite integrarse f√°cilmente con diversas herramientas y sistemas que s√≠ se encargan del procesamiento y almacenamiento de datos.

## No es bueno para‚Ä¶

Es correcto, Apache Airflow tiene limitaciones en ciertos escenarios y no siempre es la mejor soluci√≥n. A continuaci√≥n, se detallan algunos casos en los que Airflow puede no ser la opci√≥n m√°s adecuada:

### 1. **High-Frequency Sub-Minute Frequency**

Airflow no est√° dise√±ado para manejar flujos de trabajo de alta frecuencia que requieran ejecuciones sub-minuto. Su arquitectura y modelo de programaci√≥n est√°n m√°s enfocados en tareas que se ejecutan en intervalos m√°s largos, como minutos, horas o incluso d√≠as. Para tareas que necesitan ejecutarse con una frecuencia tan alta, se podr√≠an considerar otras soluciones m√°s adecuadas, como sistemas de procesamiento de eventos en tiempo real, por ejemplo, Apache Kafka o Apache Flink.

### 2. **Processing Large Datasets Directly**

Airflow no es un sistema de procesamiento de datos. No est√° dise√±ado para manejar directamente grandes vol√∫menes de datos. En su lugar, se utiliza para orquestar tareas que pueden procesar datos en sistemas externos. Si el procesamiento de grandes conjuntos de datos es una prioridad, se deben utilizar herramientas espec√≠ficas como Apache Spark, Hadoop, o sistemas de bases de datos que est√©n optimizados para manejar y procesar grandes vol√∫menes de datos de manera eficiente.

### 3. **Real-Time Data Streaming**

Airflow no est√° dise√±ado para el procesamiento de datos en tiempo real (streaming). Su enfoque est√° m√°s orientado a flujos de trabajo por lotes, donde las tareas se ejecutan en funci√≥n de una programaci√≥n establecida. Para aplicaciones que requieren un procesamiento continuo de flujos de datos en tiempo real, es mejor utilizar plataformas de streaming de datos como Apache Kafka, Apache Pulsar o Apache Flink, que est√°n dise√±adas para manejar datos en movimiento y proporcionar latencias bajas.

### 4. **Simple and Linear Workflows with Few Dependencies**

Airflow es m√°s beneficioso en flujos de trabajo complejos que involucran m√∫ltiples tareas y dependencias. Para flujos de trabajo simples y lineales con pocas o ninguna dependencia, la sobrecarga de configurar un DAG y gestionar las tareas puede ser innecesaria. En estos casos, el uso de scripts simples o herramientas m√°s ligeras podr√≠a ser suficiente y m√°s eficiente.

### Conclusi√≥n

En resumen, aunque Apache Airflow es una herramienta poderosa para la orquestaci√≥n de flujos de trabajo, no es la mejor soluci√≥n para todos los casos. En situaciones que requieren alta frecuencia, procesamiento directo de grandes conjuntos de datos, streaming en tiempo real o flujos de trabajo simples, es recomendable evaluar otras herramientas y enfoques que se adapten mejor a las necesidades espec√≠ficas del proyecto.

# Architectures

Claro, aqu√≠ tienes una versi√≥n mejorada de la respuesta que incluye tus observaciones:

## 1. Single Node Architecture

### Ubicaci√≥n de los Componentes

En una arquitectura de un solo nodo, todos los componentes de Airflow se ejecutan en una √∫nica m√°quina. Esto incluye:

- **Web Server (UI)**: Proporciona la interfaz gr√°fica para gestionar DAGs y tareas.
- **Scheduler**: Se encarga de programar las tareas.
- **Executor**: Puede ser un `LocalExecutor` o un `SequentialExecutor` que ejecuta las tareas en el mismo nodo. El Executor siempre est√° junto al Scheduler y determina c√≥mo se ejecutan las tareas.
- **Meta Database**: Almacena los metadatos sobre los DAGs y el estado de las tareas. En entornos de desarrollo, suele ser una base de datos liviana como SQLite, mientras que en producci√≥n se utilizan bases de datos m√°s robustas como PostgreSQL o MySQL.
- **Worker**: En el caso de usar `LocalExecutor`, las tareas se ejecutan como procesos hijos del Scheduler; si se usa `SequentialExecutor`, se ejecutan una a la vez.

### Comportamiento y Relaci√≥n de los Componentes

- Todos los componentes se comunican a trav√©s de la **Meta Database**, que orquesta el flujo de informaci√≥n entre ellos.
- La UI puede acceder a la base de datos para mostrar el estado y los registros de las tareas, mientras que el Scheduler utiliza la misma base de datos para programar las tareas.
- Las tareas se ejecutan localmente y, debido a la naturaleza de un solo nodo, es posible que el rendimiento se vea afectado si hay m√∫ltiples tareas que requieren recursos intensivos.

## 2. Multi Node Architecture

### Ubicaci√≥n de los Componentes

En una arquitectura de m√∫ltiples nodos, los componentes de Airflow se distribuyen en varias m√°quinas. Esta arquitectura a menudo utiliza:

- **Web Server (UI)**: Puede estar en un nodo separado, permitiendo acceso remoto a m√∫ltiples usuarios. Se pueden implementar **load balancers** para gestionar la carga y permitir que m√∫ltiples instancias del Web Server manejen solicitudes simult√°neamente.
- **Scheduler**: Se puede ejecutar en un nodo separado para manejar la programaci√≥n de tareas de manera m√°s eficiente.
- **Executor**: Frecuentemente se usa `CeleryExecutor`, que permite la ejecuci√≥n de tareas en m√∫ltiples workers distribuidos en varios nodos. Al igual que en la arquitectura de un solo nodo, el Executor siempre est√° junto al Scheduler y determina c√≥mo se ejecutan las tareas.
- **Meta Database**: Almacena los metadatos y puede estar en un nodo dedicado, utilizando una base de datos robusta como PostgreSQL o MySQL.
- **Workers**: Se distribuyen en m√∫ltiples nodos, permitiendo la ejecuci√≥n paralela de tareas.

### Comportamiento y Relaci√≥n de los Componentes

- La UI se comunica con la base de datos para mostrar informaci√≥n y permite a los usuarios gestionar los DAGs y las tareas desde cualquier lugar.
- El Scheduler, que puede estar en un nodo separado, se encarga de programar las tareas y enviar mensajes a la cola (queue) o al **message broker** (como Redis o RabbitMQ), que ser√° consumida por los Workers.
- Los Workers est√°n distribuidos, lo que permite que m√∫ltiples tareas se ejecuten en paralelo, mejorando la eficiencia y la escalabilidad.
- La separaci√≥n de componentes permite un mejor rendimiento y la posibilidad de escalar horizontalmente agregando m√°s nodos de workers seg√∫n sea necesario.

### Conclusi√≥n

Ambas arquitecturas tienen sus usos y beneficios. La arquitectura de **single node** es adecuada para entornos de desarrollo y pruebas, donde la simplicidad es clave. Por otro lado, la arquitectura de **multi node** es ideal para entornos de producci√≥n donde se requiere escalabilidad, rendimiento y la capacidad de manejar flujos de trabajo m√°s complejos con m√∫ltiples tareas. La elecci√≥n entre estas arquitecturas depender√° de los requisitos espec√≠ficos de la carga de trabajo y del entorno en el que se implementar√° Airflow.

# Operators types

Apache Airflow tiene varios tipos de operadores que se utilizan para realizar diferentes tipos de tareas en un flujo de trabajo (DAG). Aqu√≠ te muestro una clasificaci√≥n de algunos de los operadores m√°s comunes en Airflow, incluyendo **Action**, **Transfer** y **Wait**:

### 1. Action Operators

Los **Action Operators** se utilizan para realizar acciones espec√≠ficas, como ejecutar scripts, enviar correos electr√≥nicos o interactuar con otras aplicaciones. Algunos ejemplos son:

- **BashOperator**: Ejecuta comandos de Bash.
- **PythonOperator**: Ejecuta funciones de Python.
- **EmailOperator**: Env√≠a correos electr√≥nicos.
- **DockerOperator**: Ejecuta comandos dentro de un contenedor Docker.
- **PostgresOperator**: Ejecuta comandos SQL en una base de datos PostgreSQL.

### 2. Transfer Operators

Los **Transfer Operators** se utilizan para mover datos entre diferentes sistemas, como bases de datos o servicios. Algunos ejemplos son:

- **S3ToRedshiftOperator**: Transfiere datos desde Amazon S3 a Amazon Redshift.
- **MySqlToGoogleCloudStorageOperator**: Transfiere datos desde MySQL a Google Cloud Storage.
- **GoogleCloudStorageToBigQueryOperator**: Transfiere datos desde Google Cloud Storage a BigQuery.
- **CsvToHiveOperator**: Carga archivos CSV en una tabla de Hive.

### 3. Wait Operators

Los **Wait Operators** se utilizan para pausar la ejecuci√≥n de un flujo de trabajo hasta que se cumplan ciertas condiciones. Algunos ejemplos son:

- **TimeDeltaSensor**: Espera un tiempo espec√≠fico antes de continuar.
- **ExternalTaskSensor**: Espera a que un task espec√≠fico en otro DAG se complete.
- **SqlSensor**: Espera a que se cumpla una condici√≥n en una consulta SQL.
- **FileSensor**: Espera a que un archivo est√© disponible en un sistema de archivos.

### Resumen

- **Action Operators**: Realizan acciones espec√≠ficas (ej. ejecutar comandos, enviar correos electr√≥nicos).
- **Transfer Operators**: Mueven datos entre diferentes sistemas (ej. entre bases de datos, servicios en la nube).
- **Wait Operators**: Pausan la ejecuci√≥n hasta que se cumplan ciertas condiciones (ej. esperar a que un archivo est√© disponible o que un task en otro DAG se complete).

Estos operadores son fundamentales para construir flujos de trabajo complejos y eficientes en Apache Airflow. Si tienes alguna pregunta espec√≠fica sobre alguno de estos operadores o su uso, ¬°no dudes en preguntar!

# Troubleshooting con PIP

**RECUERDA QUE LA √öLTIMA VERSI√ìN DE PYTHON COMPATIBLE ES LA 3.12, NO VAYAS A USAR LA 3.13, USA EL PIP DE LA 3.12.**

Al instalar airflow y sus providers con esto (por ejemplo)

```bash
py -3.12 -m pip install apache-airflow
```

Antes necesitas instalar rust (viene con cargo) **y meter a cargo en el path, luego volver a abrir VSC**

Descarga el instalador de rust con estas opciones:

- https://rustup.rs/
- `curl --proto '=https' --tlsv1.2 -sSf [https://sh.rustup.rs](https://sh.rustup.rs/) | sh`

Luego tambi√©n instala las herramientas de desarrollo con C++ si es que una librer√≠a de google da problemas.

Aseg√∫rate de tener instalado el compilador de C++ y las herramientas necesarias para compilar extensiones. Dado que est√°s usando Visual Studio, aseg√∫rate de que tienes instalada la "C++ Build Tools". Puedes hacerlo siguiendo estos pasos:

- Abre Visual Studio Installer.
- Selecciona la instalaci√≥n de Visual Studio que tienes y haz clic en "Modificar".
- Aseg√∫rate de seleccionar "Desarrollo de escritorio con C++" o "Herramientas de desarrollo de C++".
- Completa la instalaci√≥n si es necesario.

# Connections

Las creas desde el dropdown de Admin, le metes la info que cada conexi√≥n espec√≠fica necesite. Unas necesitan puerto, otras URL, y as√≠.

# Testing

Mira el nombre del containder del scheduler con

```bash
docker-compose ps
```

M√©tete al bash del container del scheduler con

```bash
docker exec -it <nombre_scheduler> /bin/bash
```

Y luego ejecuta esto para ejecutar la tarea

```bash
airflow tasks test <DAG_id> <task_id> <past_date YYYY-MM-DD>
```

Puedes meterte al cmd de postgres estando adentro de su container con 

```bash
psql -U airflow
```

Sales de ese bash de Docker con `ctrl+D`

# Dataset y Triggers

```bash
from airflow import Dataset

my_file = Dataset(
	"s3://dataset/file.csv",
	extra={'owner': 'james'},
)
```

Puedes usarlos como trigger, as√≠ cada que otra task los modifica, se ejecutar√° a s√≠ misma.

Hay toda una pesta√±a para ver los datasets en la UI, los DAGs que los modifican, y los DAGs que son ejecutados pq tienen a ese dataset como trigger

![image.png](attachment:a16bf1b5-127f-4c5b-b3b1-8181505f325a:image.png)

![image.png](attachment:438f424c-be7f-4ab9-9b0f-e3db2c7580b6:image.png)

## **Dataset limitations**

Datasets are amazing, but they have limitations as well:

- DAGs can only use Datasets in the same Airflow instance. A DAG cannot wait for a Dataset defined in another Airflow instance.
- Consumer DAGs are triggered every time a task that updates datasets completes successfully.¬†**Airflow doesn't check whether the data has been effectively updated.**
- You can't combine different schedules like datasets with cron expressions.
- If two tasks update the same dataset, as soon as one is done, that triggers the Consumer DAG immediately without waiting for the second task to complete.
- Airflow monitors datasets only within the context of DAGs and Tasks. If an external tool updates the actual data represented by a Dataset, Airflow has no way of knowing that. (Para eso es el par√°metro del decorador, para avisar internamente. Airflow no revisa si se accedi√≥ a cierto dataset o se modific√≥, es pura comunicaci√≥n interna)

# Scheduling

```bash
# crone expressions
with DAG(schedule_interval='@daily')

# timetables
with DAG(timetable=MyTimeTable)

# since 2.4, custom?
with DAG(schedule=...)
```

# Executors

Los ejecutores de Apache Airflow son componentes clave que determinan c√≥mo se ejecutan las tareas dentro de un DAG (Directed Acyclic Graph). Cada ejecutor tiene caracter√≠sticas y casos de uso diferentes. Aqu√≠ te explico los ejecutores m√°s comunes y c√≥mo configurarlos en Airflow.

### Tipos de Ejecutores en Airflow

1. **LocalExecutor**:
    - **Descripci√≥n**: Permite la ejecuci√≥n de tareas en paralelo en la misma m√°quina donde est√° instalado Airflow. Utiliza procesos locales para ejecutar las tareas.
    - **Uso**: Ideal para entornos de desarrollo o para ejecuciones en un solo nodo.
2. **SequentialExecutor**:
    - **Descripci√≥n**: Ejecuta las tareas de manera secuencial, es decir, una a la vez. Este es el ejecutor por defecto si no se especifica otro.
    - **Uso**: √ötil para pruebas r√°pidas o en entornos donde no se necesita concurrencia.
3. **CeleryExecutor**:
    - **Descripci√≥n**: Permite la ejecuci√≥n de tareas distribuidas en m√∫ltiples nodos utilizando Celery. Las tareas se env√≠an a una cola y pueden ser procesadas por m√∫ltiples trabajadores.
    - **Uso**: Ideal para entornos de producci√≥n donde se requiere escalabilidad y alta disponibilidad.
4. **KubernetesExecutor**:
    - **Descripci√≥n**: Ejecuta las tareas en pods de Kubernetes, lo que permite escalar autom√°ticamente y aprovechar los recursos de un cl√∫ster de Kubernetes.
    - **Uso**: Perfecto para entornos en la nube y microservicios que utilizan Kubernetes.

### Configuraci√≥n de Ejecutores en Airflow (DockerCompose)

Si est√°s utilizando Docker Compose para ejecutar Apache Airflow, la configuraci√≥n se maneja un poco diferente a la instalaci√≥n est√°ndar. En este caso, los par√°metros de configuraci√≥n de Airflow se pueden establecer a trav√©s de variables de entorno en el archivo `docker-compose.yml`, lo que permite sobreescribir la configuraci√≥n del archivo `airflow.cfg`.

A continuaci√≥n, te muestro c√≥mo puedes configurar los diferentes ejecutores directamente en el archivo `docker-compose.yml`.

### Ejemplo de `docker-compose.yml`

Aqu√≠ tienes un ejemplo b√°sico de c√≥mo se configurar√≠a el ejecutor en el archivo `docker-compose.yml`:

```yaml
version: '3'

services:
  airflow-webserver:
    image: apache/airflow:2.7.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor  # Cambia a CeleryExecutor o KubernetesExecutor seg√∫n necesites
    ...

  airflow-scheduler:
    image: apache/airflow:2.7.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor  # Cambia a CeleryExecutor o KubernetesExecutor seg√∫n necesites
    ...

  airflow-worker:
    image: apache/airflow:2.7.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor  # Solo si usas CeleryExecutor
    ...

  # Si usas Redis como broker para Celery
  redis:
    image: redis:latest

  # O RabbitMQ si prefieres
  rabbit:
    image: rabbitmq:latest

```

### Configuraci√≥n de Variables de Entorno

A continuaci√≥n, se enumeran algunas variables de entorno comunes que puedes establecer en el archivo `docker-compose.yml` para configurar el ejecutor y otros par√°metros:

1. **LocalExecutor**:
    
    ```yaml
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    
    ```
    
2. **CeleryExecutor**:
Para configurar el `CeleryExecutor`, necesitar√°s establecer el broker y el backend de resultados:
    
    ```yaml
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0  # O rabbitmq://rabbit
      - AIRFLOW__CELERY__RESULT_BACKEND=db+sqlite:////usr/local/airflow/airflow.db
    
    ```
    
3. **KubernetesExecutor**:
Para el `KubernetesExecutor`, puedes configurar el contexto del cl√∫ster:
    
    ```yaml
    environment:
      - AIRFLOW__CORE__EXECUTOR=KubernetesExecutor
      - AIRFLOW__KUBERNETES__KUBE_CONFIG=/path/to/kubeconfig  # Ajusta seg√∫n tu configuraci√≥n
    
    ```
    

### Ejemplo Completo de `docker-compose.yml`

Aqu√≠ tienes un ejemplo m√°s completo de c√≥mo podr√≠a verse tu archivo `docker-compose.yml` al usar `CeleryExecutor`:

```yaml
version: '3'

services:
  airflow-webserver:
    image: apache/airflow:2.7.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+sqlite:////usr/local/airflow/airflow.db
    ...

  airflow-scheduler:
    image: apache/airflow:2.7.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+sqlite:////usr/local/airflow/airflow.db
    ...

  airflow-worker:
    image: apache/airflow:2.7.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+sqlite:////usr/local/airflow/airflow.db
    ...

  redis:
    image: redis:latest

```

### Consideraciones Finales

- **Reiniciar los Servicios**: Despu√©s de realizar cambios en el archivo `docker-compose.yml`, aseg√∫rate de reiniciar los servicios de Docker para que los cambios tengan efecto:
    
    ```bash
    docker-compose down
    docker-compose up -d
    
    ```
    
- **Persistencia de Datos**: Aseg√∫rate de que los vol√∫menes y la persistencia de datos est√©n configurados correctamente si est√°s utilizando bases de datos externas o deseas mantener datos entre reinicios.
- **Configuraciones Adicionales**: Puedes a√±adir otras configuraciones necesarias en la secci√≥n de `environment` para ajustar m√°s par√°metros de Airflow seg√∫n sea necesario.

Si tienes m√°s preguntas o necesitas ayuda adicional, ¬°h√°zmelo saber!

### Configuraci√≥n de Ejecutores en Airflow (.cfg)

Para configurar el ejecutor que deseas usar, debes modificar el archivo de configuraci√≥n de Airflow, `airflow.cfg`, que generalmente se encuentra en el directorio de Airflow (por ejemplo, `~/airflow/airflow.cfg`).

Tienes raz√≥n, si est√°s usando `docker-compose`, es mejor utilizar los comandos de `docker-compose` para interactuar con los servicios de Airflow. Aqu√≠ te muestro c√≥mo copiar el archivo `airflow.cfg` usando `docker-compose`.

### 1. Copiar `airflow.cfg` desde el Contenedor a tu M√°quina Local

Para copiar el archivo `airflow.cfg` desde un contenedor gestionado por `docker-compose`, primero necesitas identificar el nombre del servicio que contiene Airflow. Puedes ver los nombres de los servicios en tu archivo `docker-compose.yml`. Usualmente, los nombres son algo como `airflow-webserver`, `airflow-scheduler`, etc.

Puedes usar el siguiente comando con `docker-compose` para copiar el archivo:

```bash
docker-compose exec <nombre_del_servicio> cat /usr/local/airflow/airflow.cfg > ./airflow.cfg

```

Por ejemplo, si tu servicio web se llama `airflow-webserver`, el comando ser√≠a:

```bash
docker-compose exec airflow-webserver cat /usr/local/airflow/airflow.cfg > ./airflow.cfg

```

### 2. Modificar el Archivo Local

Ahora puedes abrir y modificar el archivo `airflow.cfg` en tu m√°quina local utilizando cualquier editor de texto.

### 3. Copiar el Archivo Modificado de Vuelta al Contenedor

Despu√©s de realizar los cambios en tu archivo `airflow.cfg`, puedes copiarlo de vuelta al contenedor usando el siguiente comando:

```bash
docker-compose cp ./airflow.cfg <nombre_del_servicio>:/usr/local/airflow/airflow.cfg

```

Siguiendo el mismo ejemplo, si tu servicio es `airflow-webserver`, el comando ser√≠a:

```bash
docker-compose cp ./airflow.cfg airflow-webserver:/usr/local/airflow/airflow.cfg

```

### 4. Reiniciar los Servicios de Airflow

Despu√©s de copiar el archivo `airflow.cfg` de vuelta al contenedor, es posible que necesites reiniciar los servicios de Airflow para que los cambios surtan efecto. Puedes hacerlo con:

```bash
docker-compose restart

```

### Resumen

- **Copiar desde el contenedor**: Usa `docker-compose exec <nombre_del_servicio> cat /usr/local/airflow/airflow.cfg > ./airflow.cfg`.
- **Modificar el archivo**: Edita el archivo en tu m√°quina local.
- **Copiar de vuelta al contenedor**: Usa `docker-compose cp ./airflow.cfg <nombre_del_servicio>:/usr/local/airflow/airflow.cfg`.
- **Reiniciar servicios**: Usa `docker-compose restart` para aplicar los cambios.

Gracias por tu paciencia y por se√±alar la correcci√≥n. Si necesitas m√°s ayuda, ¬°no dudes en preguntar!

### 1. LocalExecutor

Para usar el `LocalExecutor`, modifica la siguiente l√≠nea en `airflow.cfg`:

```
executor = LocalExecutor

```

### 2. SequentialExecutor

Para usar el `SequentialExecutor`, modifica la l√≠nea correspondiente:

```
executor = SequentialExecutor

```

### 3. CeleryExecutor

Para usar el `CeleryExecutor`, modifica la l√≠nea:

```
executor = CeleryExecutor

```

Adicionalmente, necesitar√°s configurar otros par√°metros relacionados con Celery, como el backend y el broker. Aseg√∫rate de tener Redis o RabbitMQ instalado y configurado. Aqu√≠ hay un ejemplo de configuraci√≥n b√°sica:

```
# Configuraci√≥n de Celery
broker_url = redis://localhost:6379/0  # Si usas Redis
result_backend = db+sqlite:///results.sqlite  # O cualquier base de datos que prefieras

```

### 4. KubernetesExecutor

Para usar el `KubernetesExecutor`, modifica la l√≠nea:

```
executor = KubernetesExecutor

```

Adem√°s, necesitar√°s configurar otros par√°metros espec√≠ficos de Kubernetes en el `airflow.cfg`, como el contexto del cl√∫ster y la conexi√≥n a la API de Kubernetes. Aseg√∫rate de tener acceso a un cl√∫ster de Kubernetes.

```
kube_config = /path/to/kubeconfig  # Ruta al archivo kubeconfig

```

### Ejemplo de Cambios en `airflow.cfg`

Aqu√≠ hay un ejemplo de c√≥mo podr√≠a verse la secci√≥n relevante del archivo `airflow.cfg`:

```
[core]
# Otras configuraciones...
executor = CeleryExecutor

# Configuraci√≥n de Celery
broker_url = redis://localhost:6379/0
result_backend = db+sqlite:///results.sqlite

[kubernetes]
kube_config = /path/to/kubeconfig

```

### Resumen

- **LocalExecutor**: Para ejecuci√≥n en un solo nodo, configurado como `executor = LocalExecutor`.
- **SequentialExecutor**: Para ejecuci√≥n secuencial, configurado como `executor = SequentialExecutor`.
- **CeleryExecutor**: Para ejecuci√≥n distribuida, configurado como `executor = CeleryExecutor` junto con la configuraci√≥n del broker y backend.
- **KubernetesExecutor**: Para ejecuci√≥n en Kubernetes, configurado como `executor = KubernetesExecutor` con par√°metros espec√≠ficos del cl√∫ster.

Despu√©s de realizar cambios en el archivo `airflow.cfg`, aseg√∫rate de reiniciar el servidor web y el scheduler de Airflow para que los cambios tengan efecto.

Si tienes m√°s preguntas o necesitas m√°s detalles sobre alguno de los ejecutores, ¬°no dudes en preguntar!

## Consideraciones al elegir DB

Al utilizar Apache Airflow, la elecci√≥n del ejecutor puede influir en las bases de datos que puedes utilizar como backend para almacenar los metadatos y resultados de las tareas. Aqu√≠ hay algunas consideraciones sobre las bases de datos y su compatibilidad con cada executor:

### 1. **LocalExecutor**

- **Base de Datos Recomendadas**:
    - **SQLite**: Ideal para desarrollo y pruebas locales. Simple de configurar, pero no es adecuado para producci√≥n debido a limitaciones de concurrencia.
    - **PostgreSQL**: Muy recomendado para producci√≥n. Soporta m√∫ltiples conexiones y proporciona robustez y rendimiento.
    - **MySQL**: Otra opci√≥n popular para producci√≥n con soporte para m√∫ltiples conexiones.
- **Consideraciones**:
    - Al usar SQLite, solo se puede ejecutar una tarea a la vez, ya que SQLite tiene limitaciones en la concurrencia.
    - Para un entorno de producci√≥n, se recomienda usar PostgreSQL o MySQL.

### 2. **SequentialExecutor**

- **Base de Datos Recomendadas**:
    - **SQLite**: Suficiente para entornos de desarrollo y pruebas.
    - **PostgreSQL**: Excelente para producci√≥n.
    - **MySQL**: Aceptable para producci√≥n.
- **Consideraciones**:
    - **Limitaciones de Concurrencia**: Solo se permite la ejecuci√≥n secuencial de tareas, lo que limita su uso en producci√≥n.
    - Similar al LocalExecutor, usar SQLite solo es recomendable para desarrollo.

### 3. **CeleryExecutor**

- **Base de Datos Recomendadas**:
    - **PostgreSQL**: Muy recomendado para producci√≥n, permite la ejecuci√≥n concurrente de m√∫ltiples tareas.
    - **MySQL**: Tambi√©n adecuado para producci√≥n.
    - **SQLite**: No recomendado para producci√≥n, ya que no maneja bien la concurrencia y no es adecuado para entornos distribuidos.
- **Consideraciones**:
    - **Broker y Backend**: Necesitar√°s configurar un broker (como RabbitMQ o Redis) para manejar la cola de tareas.
    - La base de datos debe ser robusta para manejar m√∫ltiples conexiones y operaciones concurrentes.

### 4. **KubernetesExecutor**

- **Base de Datos Recomendadas**:
    - **PostgreSQL**: Ideal para entornos de producci√≥n distribuidos.
    - **MySQL**: Tambi√©n puede ser utilizado en producci√≥n.
    - **SQLite**: No recomendado, ya que no es adecuado para la concurrencia y la naturaleza distribuida del KubernetesExecutor.
- **Consideraciones**:
    - **Escalabilidad**: La base de datos debe ser capaz de escalar con el n√∫mero de pods en el cl√∫ster de Kubernetes.
    - Al igual que con el CeleryExecutor, necesitar√°s un broker para manejar las tareas.

### Resumen de Compatibilidad de Bases de Datos

| Executor | SQLite | PostgreSQL | MySQL |
| --- | --- | --- | --- |
| LocalExecutor | **S√≠** (desarrollo) | **S√≠** (producci√≥n) | **S√≠** (producci√≥n) |
| SequentialExecutor | **S√≠** (desarrollo) | **S√≠** (producci√≥n) | **S√≠** (producci√≥n) |
| CeleryExecutor | **No** | **S√≠** (producci√≥n) | **S√≠** (producci√≥n) |
| KubernetesExecutor | **No** | **S√≠** (producci√≥n) | **S√≠** (producci√≥n) |

### Consideraciones Generales

- **Conexiones Simult√°neas**: Aseg√∫rate de que la base de datos elegida pueda manejar m√∫ltiples conexiones simult√°neas, especialmente en entornos de producci√≥n.
- **Rendimiento y Escalabilidad**: Para aplicaciones en producci√≥n, PostgreSQL y MySQL son m√°s adecuados que SQLite debido a su capacidad para manejar cargas de trabajo m√°s altas y su mejor soporte para concurrencia.
- **Configuraci√≥n del Backend**: Dependiendo del ejecutor y la base de datos, puede ser necesario configurar adecuadamente el backend de resultados (especialmente para CeleryExecutor y KubernetesExecutor) para asegurar que las tareas se gestionen correctamente.

Si tienes m√°s preguntas o necesitas m√°s detalles sobre alguna base de datos o ejecutor en particular, ¬°h√°zmelo saber!

## Celery y sus componentes especiales

Cuando trabajas con el **CeleryExecutor** en Apache Airflow, se utiliza un sistema de colas de tareas basado en **Celery**. Este sistema permite la ejecuci√≥n distribuida de tareas, lo que significa que puedes ejecutar tareas en varios trabajadores simult√°neamente, aumentando la eficiencia y escalabilidad de tus flujos de trabajo. Aqu√≠ te explico los componentes de la Celery Queue y c√≥mo funciona todo en este contexto.

### Componentes de Celery Queue

1. **Celery Workers**:
    - Son los procesos que ejecutan las tareas. Puedes tener m√∫ltiples trabajadores en diferentes m√°quinas o contenedores, permitiendo que las tareas se distribuyan y se procesen en paralelo.
    - Los trabajadores escuchan la cola de tareas y ejecutan las tareas tan pronto como est√°n disponibles.
2. **Broker**:
    - Es el componente que maneja la cola de mensajes entre el productor (Airflow) y los consumidores (los trabajadores de Celery).
    - Los brokers m√°s comunes son **RabbitMQ** y **Redis**. Estos sistemas son responsables de recibir, almacenar y enviar mensajes (tareas) entre Airflow y los trabajadores.
    - El broker asegura que las tareas se mantengan en cola hasta que un trabajador est√© disponible para procesarlas.
3. **Backend de Resultados**:
    - Se utiliza para almacenar los resultados de las tareas ejecutadas. Permite a Airflow obtener el estado y los resultados de las tareas para su posterior uso.
    - Puedes usar bases de datos como PostgreSQL, MySQL o incluso Redis como backend de resultados.
4. **Airflow Scheduler**:
    - Es el componente que crea las tareas y las env√≠a a la cola del broker. El scheduler es responsable de planificar y gestionar la ejecuci√≥n de las tareas dentro de los DAG (Directed Acyclic Graphs).
    - Cuando el scheduler determina que una tarea debe ejecutarse (por ejemplo, cuando una tarea anterior ha terminado), env√≠a un mensaje a la cola para que un trabajador lo recoja.
5. **Airflow Web Server**:
    - Proporciona una interfaz web para visualizar y gestionar los DAGs y las tareas.
    - Permite ver el estado de las tareas y los resultados, as√≠ como interactuar con los trabajos en ejecuci√≥n.

### Funcionamiento del CeleryExecutor

Aqu√≠ tienes un resumen sobre c√≥mo funciona todo el flujo de trabajo cuando usas el **CeleryExecutor**:

1. **Definici√≥n del DAG**:
    - Creas un DAG en Airflow y defines las tareas que deseas ejecutar. Cada tarea puede ser una funci√≥n o un operador que realiza una acci√≥n espec√≠fica.
2. **Scheduler**:
    - El scheduler de Airflow eval√∫a el estado de los DAGs y determina cu√°ndo deben ejecutarse las tareas. Cuando es el momento de ejecutar una tarea, el scheduler env√≠a un mensaje a la cola gestionada por el broker.
3. **Broker**:
    - El mensaje de la tarea se coloca en la cola del broker (RabbitMQ o Redis). Este broker act√∫a como intermediario que gestiona la comunicaci√≥n entre Airflow y los trabajadores.
4. **Workers**:
    - Los trabajadores de Celery est√°n en ejecuci√≥n y est√°n constantemente escuchando la cola del broker. Cuando un trabajador detecta un nuevo mensaje (una tarea), lo recoge y comienza a ejecutarlo.
5. **Ejecuci√≥n de Tareas**:
    - Mientras se ejecuta la tarea, el trabajador puede enviar peri√≥dicamente el estado de la tarea de vuelta al backend de resultados, lo que permite que Airflow rastree el progreso y el resultado de la tarea.
6. **Resultados**:
    - Una vez que la tarea se completa, el trabajador almacena el resultado en el backend de resultados. Airflow puede consultar este backend para obtener el estado y el resultado de las tareas.
7. **Visualizaci√≥n**:
    - A trav√©s del servidor web de Airflow, puedes ver el estado de las tareas, los DAGs, y los resultados de las tareas una vez que han sido finalizadas.

### Resumen de Flujo de Trabajo

1. **DAG definido** ‚ûî 2. **Scheduler env√≠a tarea a la cola** ‚ûî 3. **Broker almacena la tarea** ‚ûî 4. **Workers recogen la tarea** ‚ûî 5. **Workers ejecutan la tarea** ‚ûî 6. **Resultados almacenados en el backend** ‚ûî 7. **Airflow muestra el estado y resultados**.

### Consideraciones

- **Escalabilidad**: Con el **CeleryExecutor**, puedes escalar horizontalmente a√±adiendo m√°s trabajadores para manejar m√°s tareas en paralelo.
- **Configuraci√≥n del Broker**: Aseg√∫rate de que el broker est√© correctamente configurado y que los trabajadores puedan comunicarse con √©l.
- **Manejo de Errores**: Celery proporciona funcionalidades para reintentar tareas fallidas y manejar errores, lo que es √∫til en flujos de trabajo complejos.

Si tienes m√°s preguntas o necesitas informaci√≥n adicional sobre aspectos espec√≠ficos de Celery y Airflow, ¬°no dudes en preguntar!

# Flower

Es para monitorear tus workers

Lo activas con

```bash
docker-compose down
```

```bash
docker-compose --profile flower up -d
```

O reinicias el docker compose para usarlo, as√≠:

```bash
docker-compose down && docker-compose --profile flower up -d
```

Se accede por 

```bash
http://localhost:5555
```

Desde ac√° puedes crear una queue para un worker en espec√≠fico, en caso de que ese worker tenga muchos recursos y sea muy pro.

Y puedes ver las tasks, sus detalles y quiz√°s sus logs desde aqu√≠ tambi√©n

La p√°gina est√° un poco bugeada, as√≠ que recarga la p√°gina cada que salga que algo no existe o que no lo conoce

# How o remove example DAGs

**Remove DAG examples**

### Remove DAG¬†examples

To keep our Airflow instance nice and clean, we are going to remove the DAG examples from the UI

To do this

1. Open the file docker-compose.yaml
2. Replace the value 'true' by 'false' for the AIRFLOW__CORE__LOAD_EXAMPLES environment variables
    
    ![](https://img-c.udemycdn.com/redactor/raw/article_lecture/2022-07-25_16-35-29-96aeec021d86c642c08c2e13086b6e49.png)
    
3. Save the file
4. Restart Airflow by typing¬†`docker-compose down && docker-compose up -d`
    
    ![](https://img-c.udemycdn.com/redactor/raw/article_lecture/2022-07-25_16-35-29-c447bcd8d817a61d95b2180fa57be299.png)
    
5. Once it's done, go back to localhost:8080 and you should end up with something like this
    
    ![](https://img-c.udemycdn.com/redactor/raw/article_lecture/2022-07-25_16-35-30-5b757e96965f5db00c9bf0f778a81f85.png)
    

# Queues

En Apache Airflow, el concepto de **queues** (colas) es fundamental, especialmente cuando se utiliza el **CeleryExecutor**. Las colas permiten gestionar la distribuci√≥n de tareas entre m√∫ltiples trabajadores, facilitando la escalabilidad y el manejo eficiente de las cargas de trabajo. A continuaci√≥n, te proporciono una visi√≥n completa sobre el uso de colas en Airflow, sus ventajas y consideraciones.

### ¬øQu√© son las Queues en Airflow?

En el contexto de Airflow y Celery, una cola es un sistema que organiza las tareas que deben ser ejecutadas por los trabajadores. Las tareas se env√≠an a estas colas y los trabajadores las recogen y las ejecutan.

### Uso de Queues en Airflow

1. **Distribuci√≥n de Carga**:
    - Las colas permiten distribuir tareas entre varios trabajadores. Esto es especialmente √∫til en entornos de producci√≥n donde se requiere manejar una gran cantidad de tareas simult√°neamente.
2. **Prioridades**:
    - Puedes definir m√∫ltiples colas y asignar diferentes prioridades a las tareas. Esto permite que algunas tareas se procesen antes que otras, seg√∫n su importancia.
3. **Aislamiento de Tareas**:
    - Al utilizar diferentes colas, puedes aislar ciertos tipos de tareas. Por ejemplo, puedes tener una cola para tareas cr√≠ticas y otra para tareas de menor prioridad.
4. **Configuraci√≥n de DAGs**:
    - En Airflow, puedes especificar la cola en la que una tarea debe ejecutarse al definir el operador. Esto se puede hacer utilizando el par√°metro `queue` en los operadores.

### Ventajas de Usar Queues

1. **Escalabilidad**:
    - Permiten escalar horizontalmente, a√±adiendo m√°s trabajadores para manejar m√°s tareas a medida que la carga de trabajo aumenta.
2. **Flexibilidad**:
    - Puedes ajustar el n√∫mero de trabajadores y la configuraci√≥n de las colas sin necesidad de reescribir el c√≥digo de tus DAGs. Esto proporciona flexibilidad en la gesti√≥n de recursos.
3. **Optimizaci√≥n de Recursos**:
    - Al asignar tareas a diferentes colas, puedes optimizar el uso de los recursos. Por ejemplo, puedes dedicar m√°s recursos a tareas que requieren m√°s tiempo de procesamiento y menos a tareas r√°pidas.
4. **Control de Prioridades**:
    - Puedes gestionar la prioridad de las tareas, asegurando que las tareas cr√≠ticas se procesen primero.
5. **Manejo de Errores**:
    - Las colas permiten manejar tareas fallidas de manera m√°s efectiva. Puedes configurar reintentos y redirigir tareas fallidas a colas espec√≠ficas para su posterior an√°lisis o reejecuci√≥n.

### C√≥mo Definir Queues en Airflow

Para definir y usar colas en Airflow, sigue estos pasos:

1. **Definir Colas en el Broker**:
    - Si est√°s usando un broker como RabbitMQ o Redis, aseg√∫rate de que las colas est√©n correctamente configuradas en el broker.
2. **Especificar la Cola en las Tareas**:
    - Al definir una tarea en tu DAG, puedes especificar la cola que debe usar. Por ejemplo:
    
    ```python
    from airflow import DAG
    from airflow.operators.dummy_operator import DummyOperator
    from datetime import datetime
    
    default_args = {
        'owner': 'airflow',
        'start_date': datetime(2023, 10, 1),
    }
    
    dag = DAG('example_dag', default_args=default_args, schedule_interval='@daily')
    
    task1 = DummyOperator(
        task_id='task1',
        queue='high_priority',  # Especifica la cola
        dag=dag,
    )
    
    task2 = DummyOperator(
        task_id='task2',
        queue='low_priority',  # Especifica otra cola
        dag=dag,
    )
    
    ```
    
3. **Configuraci√≥n del Worker**:
    - Al iniciar los trabajadores de Celery, aseg√∫rate de que est√©n configurados para escuchar las colas necesarias. Por ejemplo, puedes iniciar un trabajador con:
    
    ```bash
    celery -A airflow.executors.celery_executor worker --loglevel=info --queues=high_priority
    
    ```
    

### Consideraciones

- **Configuraci√≥n de Broker**: Aseg√∫rate de que el broker que est√°s utilizando soporte m√∫ltiples colas y que est√© configurado adecuadamente.
- **Manejo de Tareas**: Si una tarea no se procesa, verifica que el trabajador est√© escuchando la cola correcta y que no haya problemas de conexi√≥n con el broker.
- **Monitoreo**: Utiliza herramientas como **Flower** para monitorear y gestionar las colas y los trabajadores. Esto te permitir√° tener una visi√≥n clara del estado de las tareas.

### Resumen

Las colas en Airflow son una herramienta poderosa para gestionar la ejecuci√≥n de tareas en entornos distribuidos, especialmente cuando se utiliza el **CeleryExecutor**. Proporcionan escalabilidad, flexibilidad, optimizaci√≥n de recursos y control de prioridades, lo que mejora la eficiencia general del sistema. Al definir y gestionar colas adecuadamente, puedes mejorar significativamente el rendimiento de tus flujos de trabajo en Airflow.

Si tienes m√°s preguntas o necesitas informaci√≥n adicional sobre colas en Airflow, ¬°no dudes en preguntar!

# Adding Workers

Claro, aqu√≠ tienes una gu√≠a m√°s completa sobre c√≥mo agregar trabajadores (workers) a un cl√∫ster de Celery en Apache Airflow creando un servicio diferente, as√≠ como utilizando el comando `airflow celery worker`.

### 1. **Configuraci√≥n del Entorno**

Antes de agregar trabajadores, aseg√∫rate de que tu entorno de Airflow y Celery est√© configurado correctamente:

- Debes tener un **broker** (como RabbitMQ o Redis) en funcionamiento.
- Aseg√∫rate de que el **backend de resultados** est√© configurado y sea accesible para los trabajadores.

### 2. **Agregar un Worker de Celery**

### 2.1. Poni√©ndole m√°s r√©plicas al servicio

```yaml
version: '3'

services:
  airflow-webserver:
    image: apache/airflow:2.7.0
    ...

  airflow-scheduler:
    image: apache/airflow:2.7.0
    ...

  airflow-worker:
    image: apache/airflow:2.7.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+sqlite:////usr/local/airflow/airflow.db
    deploy:
      replicas: 3  # N√∫mero de r√©plicas (workers)

  redis:
    image: redis:latest
```

### **2.2. Creando un Servicio Diferente**

Si est√°s utilizando Docker Compose, puedes agregar un nuevo servicio espec√≠ficamente para el worker de Celery en tu archivo `docker-compose.yml`. Aqu√≠ hay un ejemplo:

```yaml
version: '3'

services:
  airflow-webserver:
    image: apache/airflow:2.7.0
    ...

  airflow-scheduler:
    image: apache/airflow:2.7.0
    ...

  airflow-worker-1:
    image: apache/airflow:2.7.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+sqlite:////usr/local/airflow/airflow.db
    deploy:
      replicas: 1  # Puedes cambiar esto seg√∫n tus necesidades

  airflow-worker-2:
    image: apache/airflow:2.7.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+sqlite:////usr/local/airflow/airflow.db
    deploy:
      replicas: 1  # Un segundo worker

  redis:
    image: redis:latest

```

```yaml
# Realmente un servicio de workers se ve as√≠
# mejor copia y pega este
airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      # yamllint disable rule:line-length
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
```

En este ejemplo, hemos creado dos servicios de worker: `airflow-worker-1` y `airflow-worker-2`, cada uno de los cuales puede ejecutarse de forma independiente. Puedes ajustar el n√∫mero de r√©plicas para cada uno seg√∫n tus necesidades.

### Reiniciar los Servicios de Docker Compose

Si has modificado el archivo `docker-compose.yml`, aseg√∫rate de reiniciar los servicios para aplicar los cambios:

```bash
docker-compose down
docker-compose up -d

```

### 3. O u**sando el Comando `airflow celery worker`**

Otra forma de iniciar un worker de Celery es utilizando el comando `airflow celery worker`. Este m√©todo es √∫til si no est√°s utilizando Docker o si deseas iniciar un worker manualmente desde la l√≠nea de comandos.

### Paso a Paso:

1. **Accede a tu entorno de Airflow** (puede ser un contenedor o una m√°quina virtual):
    
    ```bash
    # Si est√°s en un contenedor de Docker, usa:
    docker-compose exec airflow-worker bash
    
    ```
    
2. **Ejecuta el Comando para Iniciar el Worker**:
    
    ```bash
    airflow celery worker --log-level=info
    
    ```
    
    o
    
    ```yaml
    celery -A airflow.executors.celery_executor worker --loglevel=info
    ```
    
    Este comando iniciar√° un worker de Celery que escuchar√° las colas configuradas en tu entorno de Airflow.
    

### 4. **Configuraci√≥n de Workers para Escuchar Colas Espec√≠ficas**

Si deseas que un worker escuche a una cola espec√≠fica, puedes hacerlo usando el par√°metro `--queues` al iniciar el worker. Por ejemplo:

```bash
airflow celery worker --log-level=info --queues=high_priority

```

O

```yaml
celery -A airflow.executors.celery_executor worker --loglevel=info --queues=high_priority
```

Esto har√° que el worker solo procese tareas que se env√≠en a la cola `high_priority`.

O si lo quieres hacer desde la creaci√≥n del servicio, puedes agregarle `-q <queue_name>` al final del command del servicio (`command: celery worke`r)

```yaml
airflow-worker:
    <<: *airflow-common
    command: celery worker -q high_cpu
    healthcheck:
      # yamllint disable rule:line-length
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
```

### 5. **Monitoreo de Workers**

Puedes usar **Flower** para monitorear el estado de tus workers y las tareas que est√°n procesando. Si ya tienes Flower configurado, accede a √©l a trav√©s de `http://localhost:5555` para ver el estado de tus trabajadores y colas.

### Resumen

- **Agregar Workers**:
    - Puedes agregar workers en Docker Compose creando servicios separados para cada worker.
    - Tambi√©n puedes usar el comando `airflow celery worker` para iniciar un worker manualmente desde la l√≠nea de comandos.
- **Escucha de Colas**: Puedes especificar colas para que los workers escuchen usando el par√°metro `-queues`.
- **Reiniciar Docker Compose**: No olvides reiniciar los servicios si modificas el archivo `docker-compose.yml`.

Si tienes m√°s preguntas o necesitas m√°s detalles sobre la adici√≥n de workers a Celery en Airflow, ¬°no dudes en preguntar!

# Concurrency details

**Concurrency, the parameters you must know!**

### Concurrency, the parameters you must know!

Airflow has several parameters to tune your tasks and DAGs concurrency.

**Concurrency**¬†defines the number of tasks and DAG Runs that you can execute at the same time (in parallel)

*Starting from the configuration settings*

**parallelism / AIRFLOW__CORE__PARALELISM**

This defines the maximum number of task instances that can run in Airflow per scheduler. By default, you can execute up to 32 tasks at the same time. If you have 2 schedulers: 2 x 32 = 64 tasks.

What value to define here depends on the resources you have and the number of schedulers running.

**max_active_tasks_per_dag / AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG**

This defines the maximum number of task instances allowed to run concurrently in each DAG. By default, you can execute up to 16 tasks at the same time for a given DAG across all DAG Runs.

**max_active_runs_per_dag / AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG**

This defines the maximum number of active DAG runs per DAG. By default, you can have up to 16 DAG runs per DAG running at the same time.

# XComs

Los **XComs** (short for "cross-communications") en Apache Airflow son una forma de intercambiar peque√±os vol√∫menes de datos entre tareas en un DAG (Directed Acyclic Graph). Sin embargo, el tama√±o de los XComs puede estar limitado por la base de datos que est√©s utilizando como backend para Airflow. Aqu√≠ te explico c√≥mo el tama√±o de los XComs depende de la base de datos y algunas consideraciones a tener en cuenta.

### 1. **Almacenamiento de XComs**

Los XComs se almacenan en la base de datos de Airflow en una tabla llamada `xcom`. Esta tabla tiene las siguientes columnas relevantes:

- `task_id`: ID de la tarea que cre√≥ el XCom.
- `dag_id`: ID del DAG al que pertenece la tarea.
- `execution_date`: Fecha de ejecuci√≥n del DAG.
- `key`: Clave del XCom.
- `value`: Valor del XCom.
- `timestamp`: Marca de tiempo de cuando se cre√≥ el XCom.

### 2. **Tama√±o de los XComs y Base de Datos**

El tama√±o m√°ximo que puede tener un XCom depende de la base de datos que est√©s utilizando. A continuaci√≥n, se describen algunas bases de datos comunes y sus limitaciones:

### **PostgreSQL**

- En PostgreSQL, el tipo de datos `TEXT` se utiliza para almacenar los valores de XCom.
- El tama√±o m√°ximo de un campo `TEXT` es de aproximadamente 1 GB, pero es recomendable mantener los XComs en un tama√±o significativamente menor para evitar problemas de rendimiento y uso de memoria.

### **MySQL**

- En MySQL, se utiliza el tipo `LONGTEXT` para la columna `value` en la tabla `xcom`.
- El tama√±o m√°ximo para `LONGTEXT` es de 4 GB, pero de nuevo, se aconseja mantener los XComs mucho m√°s peque√±os.

### **SQLite**

- En SQLite, el tipo de datos utilizado es `BLOB` o `TEXT`.
- El tama√±o m√°ximo de un BLOB o TEXT es de 2 GB. Sin embargo, en entornos de producci√≥n, es mejor evitar el uso de grandes cantidades de datos en SQLite, ya que no es ideal para cargas de trabajo concurrentes.

### 3. **Recomendaciones sobre el Uso de XComs**

- **Mantener el Tama√±o Peque√±o**: Es recomendable que los datos almacenados en XComs sean peque√±os y ligeros, como cadenas de texto o diccionarios simples. Datos m√°s grandes deber√≠an almacenarse en sistemas externos (como un sistema de archivos, bases de datos, o almacenamiento en la nube) y solo pasar referencias (como rutas o IDs) a trav√©s de XComs.
- **Evitar Datos Sensibles**: Debido a que los XComs son almacenados en la base de datos, evita almacenar informaci√≥n sensible o privada.
- **Uso de `XCom Push` y `XCom Pull`**: Utiliza `xcom_push` y `xcom_pull` para manejar la comunicaci√≥n entre tareas, asegur√°ndote de que solo se env√≠an datos necesarios.

### Conclusi√≥n

Los XComs son una herramienta poderosa para la comunicaci√≥n entre tareas en Airflow, pero es importante ser consciente de las limitaciones de tama√±o impuestas por la base de datos utilizada. Mantener los datos en XComs peque√±os y manejables es clave para un rendimiento √≥ptimo en tus flujos de trabajo. Si necesitas almacenar datos m√°s grandes, considera utilizar almacenamiento externo y pasar solo referencias a trav√©s de XComs.

Si tienes m√°s preguntas o necesitas m√°s aclaraciones, ¬°no dudes en preguntar!

# Trigger Rules

En Apache Airflow, los **trigger rules** (reglas de activaci√≥n) se utilizan para determinar cu√°ndo se debe ejecutar una tarea en funci√≥n del estado de sus tareas anteriores. A continuaci√≥n, se presenta una lista de las reglas de activaci√≥n disponibles en Airflow, junto con una breve descripci√≥n de cada una.

### 1. **Trigger Rules en Airflow**

1. **`all_success`** (Por defecto):
    - La tarea se ejecuta si todas las tareas anteriores han tenido √©xito.
2. **`all_failed`**:
    - La tarea se ejecuta si todas las tareas anteriores han fallado.
3. **`all_skipped`**:
    - La tarea se ejecuta si todas las tareas anteriores han sido omitidas.
4. **`one_success`**:
    - La tarea se ejecuta si al menos una de las tareas anteriores ha tenido √©xito.
5. **`one_failed`**:
    - La tarea se ejecuta si al menos una de las tareas anteriores ha fallado.
6. **`one_skipped`**:
    - La tarea se ejecuta si al menos una de las tareas anteriores ha sido omitida.
7. **`none_failed`**:
    - La tarea se ejecuta si ninguna de las tareas anteriores ha fallado.
8. **`none_success`**:
    - La tarea se ejecuta si ninguna de las tareas anteriores ha tenido √©xito.
9. **`none_skipped`**:
    - La tarea se ejecuta si ninguna de las tareas anteriores ha sido omitida.
10. **`dummy`**:
    - La tarea se ejecuta siempre, independientemente del estado de las tareas anteriores.
11. **`all_done`**:
    - La tarea se ejecuta si todas las tareas anteriores han terminado, independientemente de su estado (√©xito, fallo, omitido).

### 2. **Ejemplo de Uso de Trigger Rules**

Aqu√≠ hay un ejemplo de c√≥mo se pueden aplicar diferentes reglas de activaci√≥n en un DAG de Airflow:

```python
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def task_success():
    print("Task succeeded!")

def task_fail():
    print("Task failed!")

with DAG('example_trigger_rules', start_date=datetime(2023, 10, 1), schedule_interval='@daily') as dag:

    start = DummyOperator(task_id='start')

    success_task = PythonOperator(
        task_id='success_task',
        python_callable=task_success,
    )

    fail_task = PythonOperator(
        task_id='fail_task',
        python_callable=task_fail,
    )

    end = DummyOperator(
        task_id='end',
        trigger_rule='one_failed'  # Esta tarea se ejecutar√° si al menos una tarea anterior ha fallado
    )

    start >> [success_task, fail_task] >> end

```

### 3. **Consideraciones**

- La selecci√≥n de la regla de activaci√≥n correcta es crucial para el flujo de trabajo de tu DAG, ya que afecta c√≥mo y cu√°ndo se ejecutan las tareas.
- Las reglas de activaci√≥n se pueden definir en el argumento `trigger_rule` de cualquier operador en Airflow.

### Resumen

Las reglas de activaci√≥n en Airflow permiten un control fino sobre el flujo de trabajo y la ejecuci√≥n de tareas en funci√≥n del estado de las tareas anteriores. Conociendo estas reglas, puedes dise√±ar DAGs m√°s eficientes y adaptados a tus necesidades.

Si tienes m√°s preguntas o necesitas m√°s detalles sobre un trigger rule espec√≠fico, ¬°no dudes en preguntar!

# Hooks y Plugins

Apache Airflow es una plataforma de orquestaci√≥n de flujos de trabajo que permite definir, programar y supervisar trabajos de procesamiento de datos. En Airflow, los **plugins** y **hooks** son componentes esenciales que extienden su funcionalidad y permiten una integraci√≥n m√°s f√°cil con diversos sistemas y servicios. A continuaci√≥n, se ofrece una descripci√≥n amplia de ambos conceptos.

## Plugins en Airflow

### ¬øQu√© son los Plugins?

Los plugins en Airflow son componentes que permiten a los usuarios extender la funcionalidad de Airflow sin modificar el c√≥digo base. Los plugins pueden agregar nuevas funcionalidades, operadores, sensores, vistas, o incluso integraciones con otros servicios.

### Usos Comunes de los Plugins

1. **Operadores Personalizados**: Crear nuevos operadores que encapsulen l√≥gicas de negocio espec√≠ficas o interacciones con APIs externas.
2. **Sensores Personalizados**: Desarrollar sensores que esperen condiciones espec√≠ficas en sistemas externos antes de continuar con el flujo de trabajo.
3. **Vistas y P√°ginas**: Crear interfaces personalizadas en el UI de Airflow para mostrar informaci√≥n adicional o para facilitar la interacci√≥n del usuario.
4. **Hooks**: Los plugins pueden incluir hooks, que son clases que se conectan a servicios externos.

### Estructura de un Plugin

Un plugin en Airflow se define en un archivo Python que extiende la clase `AirflowPlugin`. La estructura b√°sica de un plugin puede verse as√≠:

```python
from airflow.plugins_manager import AirflowPlugin
from my_custom_operator import MyCustomOperator

class MyCustomPlugin(AirflowPlugin):
    name = "my_custom_plugin"
    operators = [MyCustomOperator]  # Puedes agregar operadores, sensores, etc.

```

### Ejemplo de Plugin

Supongamos que deseas crear un operador que env√≠e un correo electr√≥nico. Tu plugin podr√≠a verse as√≠:

```python
from airflow.plugins_manager import AirflowPlugin
from airflow.operators.email_operator import EmailOperator

class CustomEmailOperator(EmailOperator):
    # Aqu√≠ puedes personalizar tu operador

class MyCustomPlugin(AirflowPlugin):
    name = "custom_email_plugin"
    operators = [CustomEmailOperator]

```

## Hooks en Airflow

### ¬øQu√© son los Hooks?

Los hooks en Airflow son clases que proporcionan una interfaz de conexi√≥n a sistemas externos, como bases de datos, servicios en la nube, APIs, o cualquier otro sistema que Airflow necesite interactuar. Los hooks encapsulan la l√≥gica de conexi√≥n y permiten que los operadores y sensores se enfoquen en la l√≥gica espec√≠fica de su tarea, delegando la gesti√≥n de conexiones a los hooks.

### Usos Comunes de los Hooks

1. **Conexiones a Bases de Datos**: Proveer una interfaz para conectarse a diferentes bases de datos (PostgreSQL, MySQL, etc.).
2. **Integraciones de Servicios**: Facilitar la conexi√≥n a servicios como AWS, Google Cloud, o APIs externas.
3. **Manejo de Errores**: Proporcionar l√≥gica para manejar errores de conexi√≥n o recuperaci√≥n de datos.

### Ejemplo de un Hook

A continuaci√≥n, se presenta un ejemplo de un hook que se conecta a una base de datos PostgreSQL:

```python
from airflow.hooks.base_hook import BaseHook
import psycopg2

class PostgresHook(BaseHook):
    def __init__(self, postgres_conn_id='postgres_default'):
        self.conn_id = postgres_conn_id
        self.connection = self.get_connection(self.conn_id)

    def get_conn(self):
        # M√©todo para obtener la conexi√≥n
        return psycopg2.connect(
            host=self.connection.host,
            database=self.connection.schema,
            user=self.connection.login,
            password=self.connection.password,
            port=self.connection.port
        )

```

### Integraci√≥n de Hooks en Operadores

Los hooks son utilizados com√∫nmente dentro de los operadores. Por ejemplo, un operador que necesita acceder a una base de datos puede utilizar un hook para obtener la conexi√≥n y ejecutar consultas.

```python
class MyDatabaseOperator(BaseOperator):
    def execute(self, context):
        hook = PostgresHook()
        conn = hook.get_conn()
        # Ejecutar operaciones en la base de datos

```

## Resumen

- **Plugins**: Permiten extender la funcionalidad de Airflow y agregar nuevos componentes como operadores, sensores y vistas. Se definen en archivos Python y son esenciales para personalizar Airflow seg√∫n las necesidades del usuario.
- **Hooks**: Proporcionan una forma de conectarse a sistemas externos, encapsulando la l√≥gica de conexi√≥n y permitiendo que otros componentes (como operadores y sensores) se enfoquen en su l√≥gica espec√≠fica.

Ambos conceptos son fundamentales para aprovechar al m√°ximo la extensibilidad y la flexibilidad de Apache Airflow, permitiendo a los usuarios personalizar sus flujos de trabajo y adaptarse a diferentes entornos y requisitos.

## Ver si se agregaron

```bash
docker-compose -f docker-compose-es.yaml ps

docker exec -it <scheduler_container_name> /bin/bash
```

# Extras

## DockerOperator

- https://marclamberti.com/blog/how-to-use-dockeroperator-apache-airflow/
- https://www.youtube.com/watch?app=desktop&v=MdW2ZHHJWeo

## Kubernetes Executor

https://marclamberti.com/blog/airflow-kubernetes-executor/

## Templates and Macros

https://marclamberti.com/blog/templates-macros-apache-airflow/

## Timezones

https://marclamberti.com/blog/how-to-use-timezones-in-apache-airflow/

## BashOperator a fondo

https://marclamberti.com/blog/airflow-bashoperator/

## Variables

https://marclamberti.com/blog/variables-with-apache-airflow/

## Buenas pr√°cticas

https://marclamberti.com/blog/apache-airflow-best-practices-1/

## Airflow en kubernetes multinodo local

https://marclamberti.com/blog/running-apache-airflow-locally-on-kubernetes/

## PostgresOperator a fondo

https://marclamberti.com/blog/the-postgresoperator-all-you-need-to-know/

## **Airflow Data Pipeline with AWS and Snowflake**

https://www.youtube.com/watch?v=wT67h9qDl1o